{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jub35xXXVE5n"
   },
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPzm7LqS8_w9"
   },
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XX8Jmj72vc0N"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, BatchNormalization\n",
    "from tensorflow import keras \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM8SnM4izO4U"
   },
   "source": [
    "#### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqBk8sbpklsV"
   },
   "outputs": [],
   "source": [
    "#Choose whether you will run the ETL or not\n",
    "\n",
    "choice = 0; #1 will run the ETL\n",
    "\n",
    "save_model = 1; #1 will save a new model in .h5 format\n",
    "\n",
    "prediction_file = \"\"; #File which prediction will occur\n",
    "\n",
    "#If choice is one, choose the portion size of train and test\n",
    "\n",
    "SIZE = 250000 #Size of the selected rows for training purposes\n",
    "\n",
    "if choice == 1:  \n",
    "  print(\"Setup for test size is:\", SIZE)\n",
    "  print(\"Setup for train size is:\", int(SIZE+SIZE*0.2))\n",
    "else:\n",
    "  print(\"Skipping the ETL on next step...\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHY5rOWwN4Dp"
   },
   "outputs": [],
   "source": [
    "if choice == 1:\n",
    "  \n",
    "  files_no = glob.glob('normal/*.csv')\n",
    "  files_no.sort() #In-list sorting\n",
    "  folders_im = glob.glob('imbalance/*')\n",
    "  folders_im.sort() #In-list sorting\n",
    "  train_data = np.empty((0,8), float)\n",
    "  test_data = np.empty((0,8), float)\n",
    "\n",
    "  for f_on in files_no:\n",
    "    source_data = np.loadtxt(f_on, delimiter=\",\")\n",
    "    train_data = np.append(train_data, source_data[0:SIZE,:], axis=0)\n",
    "    test_data = np.append(test_data, source_data[SIZE:int(SIZE+SIZE*0.25),:], axis=0)\n",
    "\n",
    "  n_train_labels = len(train_data)\n",
    "  n_test_labels = len(test_data)\n",
    "  \n",
    "  SIZE, MOD = divmod(SIZE,6)\n",
    "  \n",
    "  for folder in folders_im:\n",
    "    files_im = glob.glob( folder +'/*.csv')\n",
    "    files_im.sort() #In-list sorting\n",
    "    for f_im in files_im:\n",
    "      source_data = np.loadtxt(f_im, delimiter=\",\")\n",
    "      train_data = np.append(train_data, source_data[0:SIZE,:], axis=0)\n",
    "      test_data = np.append(test_data, source_data[SIZE:int(SIZE+SIZE*0.25),:], axis=0)\n",
    "  \n",
    "  np.savetxt(\"train_data.csv\", train_data, delimiter=\",\")\n",
    "  np.savetxt(\"test_data.csv\", test_data, delimiter=\",\")\n",
    "  \n",
    "  train_label = np.zeros(n_train_labels)\n",
    "  train_label = np.append(train_label, np.ones(len(train_data)-n_train_labels), axis=0)\n",
    "  test_label = np.zeros(n_test_labels)\n",
    "  test_label = np.append(test_label, np.ones(len(test_data)-n_test_labels), axis=0)\n",
    "  \n",
    "  np.savetxt(\"train_label.csv\", train_label, delimiter=\",\")\n",
    "  np.savetxt(\"test_label.csv\", test_label, delimiter=\",\")\n",
    "  print(\"Finished parsing the files\")\n",
    "\n",
    "else:\n",
    "  print(\"Skipping this step...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y_PzTqDxTYH"
   },
   "outputs": [],
   "source": [
    "#Loading data to variables\n",
    "if choice == 0:\n",
    "  train_data = np.loadtxt('train_data.csv', delimiter=\",\")\n",
    "  test_data = np.loadtxt('test_data.csv', delimiter=\",\")\n",
    "  train_label = np.loadtxt('train_label.csv', delimiter=\",\")\n",
    "  test_label = np.loadtxt('test_label.csv', delimiter=\",\")\n",
    "  print(\"Loaded data files...\")\n",
    "\n",
    "else:\n",
    "  print(\"Skipped loading files...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0pYIpnhmKYl"
   },
   "outputs": [],
   "source": [
    "#Checking the 'train' and 'test' shapes\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHcVWl5YlYIP"
   },
   "outputs": [],
   "source": [
    "#Applying MinMaxScaler scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.fit_transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H54hU-d5jHA"
   },
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "\n",
    "if save_model == 0:\n",
    "  model = keras.models.load_model('mafaulda.h5')\n",
    "  print(\"Model loaded...\")\n",
    "else:\n",
    "  print(\"Skipping loading model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oRuGxQEjYiE"
   },
   "outputs": [],
   "source": [
    "input_size,input_len = train_data.shape\n",
    "input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VQ-CKc38V6-"
   },
   "outputs": [],
   "source": [
    "#LSTM Model\n",
    "if save_model == 1:\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(12, input_shape=(1, 8),activation='tanh', recurrent_activation='sigmoid',return_sequences=True, return_state=False))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(LSTM(8, activation='tanh', recurrent_activation='sigmoid'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dense(64))\n",
    "  model.add(Dense(12))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  print(\"Model created...\")\n",
    "else:\n",
    "  print(\"Skipped model creation...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN_zDxb99OBY"
   },
   "outputs": [],
   "source": [
    "#Reshapping the data\n",
    "train_LSTM = train_data.reshape((train_data.shape[0], 1, train_data.shape[1]))\n",
    "test_LSTM = test_data.reshape((test_data.shape[0], 1, test_data.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k62OL_qjeJcq"
   },
   "outputs": [],
   "source": [
    "#Displaying 'train' and 'test'shapes\n",
    "display(\"Train shape\", train_LSTM.shape)\n",
    "print(\"\\n\")\n",
    "display(\"Test shape\", test_LSTM.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ks-eTfkM8tLl"
   },
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "if save_model == 1:\n",
    "  history = model.fit(train_LSTM, train_label, epochs=10, batch_size=5, validation_data=(test_LSTM, test_label), verbose=2, shuffle=False)\n",
    "else:\n",
    "  print(\"Skipping model compilation because it was loaded from a saved one.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "payZbjRgXNX-"
   },
   "outputs": [],
   "source": [
    "#Ploting Training Accuracy and Validation Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9zciJBkhTqD"
   },
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "if save_model == 1:\n",
    "  model.save('mafaulda.h5')\n",
    "  !cp mafaulda.h5 drive/MyDrive/MAFAULDA/ #Copying it to Drive\n",
    "  print(\"Model saved as mafaulda.h5\")\n",
    "else:\n",
    "  print(\"Skipping model saving...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mwu1zhrwGLhO"
   },
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(train_LSTM, train_label)\n",
    "display(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AkbHP82sMbD"
   },
   "outputs": [],
   "source": [
    "test_accuracy = model.evaluate(test_LSTM, test_label)\n",
    "display(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WrxcY2hGiTa"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCFtlfjzwlXe"
   },
   "outputs": [],
   "source": [
    "#Loading a new data to predict\n",
    "source_data = np.loadtxt(\"test_data.csv\", delimiter=\",\")\n",
    "display(source_data.shape)\n",
    "display(source_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ww5Kwk8XY0kl"
   },
   "outputs": [],
   "source": [
    "#Applying MinMaxScaler scaler to predict the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "source_data = scaler.fit_transform(source_data)\n",
    "display(source_data.shape)\n",
    "display(source_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CwG25SQAP7B"
   },
   "outputs": [],
   "source": [
    "pred_data = source_data.reshape((source_data.shape[0], 1, source_data.shape[1]))\n",
    "pred_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFZO7QZvGllK"
   },
   "outputs": [],
   "source": [
    "#Predicting data\n",
    "y = model.predict_classes(pred_data, batch_size=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mZ-1nl2B_Bc"
   },
   "outputs": [],
   "source": [
    "display(len(source_data))\n",
    "display(y.sum())\n",
    "display(y.sum()/len(source_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHXbIKBF5N9k"
   },
   "outputs": [],
   "source": [
    "#Loading a new data to predict (Normal data)\n",
    "import os\n",
    "\n",
    "dictio = {}\n",
    "\n",
    "for file in map(os.path.basename, glob.glob('normal/*')):\n",
    "  \n",
    "  source_data = np.loadtxt('normal/'+ file, delimiter=\",\")\n",
    "  source_data = source_data[0:2500,:]\n",
    "  display(source_data.shape)\n",
    "  pred_data = source_data.reshape((source_data.shape[0], 1, source_data.shape[1]))\n",
    "  y = model.predict_classes(pred_data, batch_size=5, verbose=1)\n",
    "  print(file,y.sum()/len(source_data))\n",
    "  dictio[file] = y.sum()/len(source_data)\n",
    "\n",
    "dictio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG4DQvvf2RkT"
   },
   "outputs": [],
   "source": [
    "#Loading a new data to predict (Imbalance data)\n",
    "dictio_imbalance = {}\n",
    "\n",
    "for file in map(os.path.basename, glob.glob('imbalance/25g/*')):\n",
    "  \n",
    "  source_data = np.loadtxt('imbalance/25g/'+ file, delimiter=\",\")\n",
    "  source_data = source_data[0:2500,:]\n",
    "  display(source_data.shape)\n",
    "  pred_data = source_data.reshape((source_data.shape[0], 1, source_data.shape[1]))\n",
    "  y = model.predict_classes(pred_data, batch_size=5, verbose=1)\n",
    "  print(file,y.sum()/len(source_data))\n",
    "  dictio_imbalance[file] = y.sum()/len(source_data)\n",
    "\n",
    "dictio_imbalance\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ernani_motor_fault_v1.2a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
