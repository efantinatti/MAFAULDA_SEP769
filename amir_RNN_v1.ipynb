{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9b57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN, LSTM\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd4bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice = 1 if you have access to the data but not train/test data .txt files\n",
    "#Choice = 0 if you have train/test data .txt files\n",
    "\n",
    "choice = 0\n",
    "\n",
    "#Number of data points to take per batch\n",
    "no_points = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77091c",
   "metadata": {},
   "source": [
    "Saving 3D Arrays:\n",
    "https://www.geeksforgeeks.org/how-to-load-and-save-3d-numpy-array-to-file-using-savetxt-and-loadtxt-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19425f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops through all .csv files containing normal and imbalanced data, \n",
    "#takes first 5000 points from each, assigns label in separate array\n",
    "if choice == 1:\n",
    "    files_no = glob.glob('normal/*.csv')\n",
    "    folders_im = glob.glob('imbalance/*')\n",
    "    train_data = np.empty((382,no_points,8), float)\n",
    "    test_data = np.empty((382,no_points,8), float)\n",
    "    i=0\n",
    "    for f_on in files_no:\n",
    "        source_data = np.loadtxt(f_on, delimiter=\",\")\n",
    "        train_data[i,:,:] = source_data[0:no_points,:]\n",
    "        test_data[i,:,:] = source_data[no_points:no_points*2,:]\n",
    "        i=i+1\n",
    "\n",
    "    for folder in folders_im:\n",
    "        files_im = glob.glob( folder +'/*.csv')\n",
    "        for f_im in files_im:\n",
    "            source_data = np.loadtxt(f_im, delimiter=\",\")\n",
    "            train_data[i,:,:] = source_data[0:no_points,:]\n",
    "            test_data[i,:,:] = source_data[no_points:no_points*2,:]\n",
    "            i=i+1\n",
    "    \n",
    "    train_reshaped = np.reshape(train_data,(train_data.shape[0],-1))\n",
    "    test_reshaped = np.reshape(test_data,(train_data.shape[0],-1))\n",
    "    \n",
    "    np.savetxt(\"train_data.txt\", train_reshaped)\n",
    "    np.savetxt(\"test_data.txt\", test_reshaped)\n",
    "    \n",
    "    train_label = np.zeros(49)\n",
    "    train_label = np.append(train_label, np.ones(333), axis=0)\n",
    "    test_label = np.zeros(49)\n",
    "    test_label = np.append(test_label, np.ones(333), axis=0)\n",
    "\n",
    "    np.savetxt(\"train_label.txt\", train_label)\n",
    "    np.savetxt(\"test_label.txt\", test_label)\n",
    "    \n",
    "    print(\"Finished parsing the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828eed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382,)\n",
      "Loaded data files...\n"
     ]
    }
   ],
   "source": [
    "#Loads train/test data from .txt files to np arrays\n",
    "if choice == 0:    \n",
    "    train_reload = np.loadtxt(\"train_data.txt\")\n",
    "    test_reload = np.loadtxt(\"test_data.txt\")\n",
    "\n",
    "    train_label = np.loadtxt('train_label.txt').astype(int)\n",
    "    test_label = np.loadtxt('test_label.txt').astype(int)\n",
    "    \n",
    "    print(train_label.shape)\n",
    "    print(\"Loaded data files...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22271df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382, 5000, 8)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.reshape(train_reload,(train_reload.shape[0],train_reload.shape[1]//8, 8))\n",
    "test_data = np.reshape(test_reload,(test_reload.shape[0],test_reload.shape[1]//8, 8))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5751e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38200, 50, 8)\n"
     ]
    }
   ],
   "source": [
    "#Splits the batch of 5000 into ten batches of 50\n",
    "train_data = train_data.reshape(-1,int(no_points/100),8)\n",
    "test_data = test_data.reshape(-1, int(no_points/100), 8)\n",
    "train_label = np.zeros(4900)\n",
    "train_label = np.append(train_label, np.ones(33300), axis=0)\n",
    "test_label = np.zeros(4900)\n",
    "test_label = np.append(test_label, np.ones(33300), axis=0)\n",
    "\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16153052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 50, 12)            1008      \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 4)                 272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 1,285\n",
      "Trainable params: 1,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(12, input_shape = (50,8), activation = 'tanh', recurrent_activation='sigmoid', return_sequences=False))\n",
    "# model.add(LSTM(4, activation = 'tanh', recurrent_activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe16fe62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "764/764 [==============================] - 14s 16ms/step - loss: 0.1801 - accuracy: 0.9135 - val_loss: 0.1896 - val_accuracy: 0.9091\n",
      "Epoch 2/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1669 - accuracy: 0.9196 - val_loss: 0.1668 - val_accuracy: 0.9410\n",
      "Epoch 3/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1513 - accuracy: 0.9423 - val_loss: 0.1595 - val_accuracy: 0.9366\n",
      "Epoch 4/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1474 - accuracy: 0.9439 - val_loss: 0.1701 - val_accuracy: 0.9380\n",
      "Epoch 5/20\n",
      "764/764 [==============================] - 11s 14ms/step - loss: 0.1419 - accuracy: 0.9435 - val_loss: 0.1486 - val_accuracy: 0.9416\n",
      "Epoch 6/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1344 - accuracy: 0.9486 - val_loss: 0.1541 - val_accuracy: 0.9380\n",
      "Epoch 7/20\n",
      "764/764 [==============================] - 11s 14ms/step - loss: 0.1308 - accuracy: 0.9504 - val_loss: 0.1386 - val_accuracy: 0.9475\n",
      "Epoch 8/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1253 - accuracy: 0.9526 - val_loss: 0.1335 - val_accuracy: 0.9506\n",
      "Epoch 9/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1248 - accuracy: 0.9526 - val_loss: 0.1300 - val_accuracy: 0.9537\n",
      "Epoch 10/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1208 - accuracy: 0.9544 - val_loss: 0.1329 - val_accuracy: 0.9520\n",
      "Epoch 11/20\n",
      "764/764 [==============================] - 11s 14ms/step - loss: 0.1187 - accuracy: 0.9549 - val_loss: 0.1452 - val_accuracy: 0.9493\n",
      "Epoch 12/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1623 - accuracy: 0.9238 - val_loss: 0.1599 - val_accuracy: 0.9269\n",
      "Epoch 13/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1289 - accuracy: 0.9486 - val_loss: 0.1320 - val_accuracy: 0.9480\n",
      "Epoch 14/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1236 - accuracy: 0.9509 - val_loss: 0.1310 - val_accuracy: 0.9513\n",
      "Epoch 15/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1225 - accuracy: 0.9506 - val_loss: 0.1280 - val_accuracy: 0.9516\n",
      "Epoch 16/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1124 - accuracy: 0.9563 - val_loss: 0.1230 - val_accuracy: 0.9549\n",
      "Epoch 17/20\n",
      "764/764 [==============================] - 12s 15ms/step - loss: 0.1111 - accuracy: 0.9572 - val_loss: 0.1280 - val_accuracy: 0.9501\n",
      "Epoch 18/20\n",
      "764/764 [==============================] - 12s 15ms/step - loss: 0.1150 - accuracy: 0.9562 - val_loss: 0.1256 - val_accuracy: 0.9522\n",
      "Epoch 19/20\n",
      "764/764 [==============================] - 12s 15ms/step - loss: 0.1073 - accuracy: 0.9623 - val_loss: 0.1235 - val_accuracy: 0.9548\n",
      "Epoch 20/20\n",
      "764/764 [==============================] - 11s 15ms/step - loss: 0.1121 - accuracy: 0.9596 - val_loss: 0.1185 - val_accuracy: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ed9284b190>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_label, validation_data=(test_data, test_label), epochs=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4fd957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 17ms/step - loss: 0.3181 - accuracy: 0.8819\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2b6bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 8)\n"
     ]
    }
   ],
   "source": [
    "normal_load = np.loadtxt(\"normal/61.44.csv\", delimiter = ',')\n",
    "# normal_load = np.loadtxt(\"imbalance/35g/56.7296.csv\", delimiter = ',')\n",
    "normal_data = normal_load[200000:250000,:]\n",
    "print(normal_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c57b55dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\\12.288.csv [0.1494]\n",
      "normal\\13.1072.csv [0.1928]\n",
      "normal\\14.336.csv [0.2368]\n",
      "normal\\15.1552.csv [0.211]\n",
      "normal\\16.1792.csv [0.1974]\n",
      "normal\\17.2032.csv [0.2008]\n",
      "normal\\18.432.csv [0.17]\n",
      "normal\\19.6608.csv [0.1604]\n",
      "normal\\20.2752.csv [0.1376]\n",
      "normal\\21.7088.csv [0.179]\n",
      "normal\\22.3232.csv [0.1866]\n",
      "normal\\23.552.csv [0.1404]\n",
      "normal\\24.576.csv [0.139]\n",
      "normal\\25.6.csv [0.1254]\n",
      "normal\\26.624.csv [0.1596]\n",
      "normal\\27.4432.csv [0.2366]\n",
      "normal\\28.8768.csv [0.1364]\n",
      "normal\\29.4912.csv [0.1776]\n",
      "normal\\30.72.csv [0.1436]\n",
      "normal\\31.744.csv [0.1278]\n",
      "normal\\32.9728.csv [0.1218]\n",
      "normal\\33.5872.csv [0.1222]\n",
      "normal\\34.2016.csv [0.1522]\n",
      "normal\\35.4304.csv [0.0998]\n",
      "normal\\36.4544.csv [0.095]\n",
      "normal\\37.6832.csv [0.0956]\n",
      "normal\\38.2976.csv [0.0838]\n",
      "normal\\39.3216.csv [0.0756]\n",
      "normal\\40.3456.csv [0.1356]\n",
      "normal\\41.7792.csv [0.1234]\n",
      "normal\\42.3936.csv [0.1104]\n",
      "normal\\43.6224.csv [0.104]\n",
      "normal\\44.6464.csv [0.1038]\n",
      "normal\\45.4656.csv [0.112]\n",
      "normal\\46.2848.csv [0.0938]\n",
      "normal\\47.7184.csv [0.1138]\n",
      "normal\\48.9472.csv [0.0996]\n",
      "normal\\49.5616.csv [0.2812]\n",
      "normal\\51.8144.csv [0.2042]\n",
      "normal\\52.4288.csv [0.2512]\n",
      "normal\\53.8624.csv [0.2052]\n",
      "normal\\54.6816.csv [0.2296]\n",
      "normal\\55.7056.csv [0.1802]\n",
      "normal\\56.7296.csv [0.2026]\n",
      "normal\\57.9584.csv [0.1538]\n",
      "normal\\58.7776.csv [0.208]\n",
      "normal\\59.5968.csv [0.3596]\n",
      "normal\\60.416.csv [0.4454]\n",
      "normal\\61.44.csv [0.389]\n"
     ]
    }
   ],
   "source": [
    "files_no = glob.glob('normal/*.csv')\n",
    "# errors = numpy.empty(1)\n",
    "for f_on in files_no:\n",
    "    normal_data = np.loadtxt(f_on, delimiter=\",\")\n",
    "    normal_data = normal_data.reshape(-1,50,8)\n",
    "    y = model.predict_classes(normal_data)\n",
    "    print(f_on, sum(y)/y.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d4261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load prediction data\n",
    "pred_load = np.loadtxt(\"pred_data.txt\")\n",
    "pred_label = np.zeros(490*500)\n",
    "pred_label = np.append(pred_label, np.ones(3330*500), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eae0b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 500, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "59688/59688 [==============================] - 197s 3ms/step - loss: 0.6386 - accuracy: 0.8145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6385664343833923, 0.8144764304161072]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = np.reshape(pred_load,(-1, 1, 8))\n",
    "model.evaluate(pred_data, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f3c831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.56815   0.60986   0.034358  0.033138 -0.63896   0.016935 -0.17671\n",
      "  0.022027]\n"
     ]
    }
   ],
   "source": [
    "print(pred_data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdd8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu-new]",
   "language": "python",
   "name": "conda-env-tf-gpu-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
