{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a9b57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fd4bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice = 1 if you have access to the data but not train/test data .txt files\n",
    "#Choice = 0 if you have train/test data .txt files\n",
    "\n",
    "choice = 0\n",
    "\n",
    "#Number of data points to take per batch\n",
    "no_points = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77091c",
   "metadata": {},
   "source": [
    "Saving 3D Arrays:\n",
    "https://www.geeksforgeeks.org/how-to-load-and-save-3d-numpy-array-to-file-using-savetxt-and-loadtxt-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19425f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops through all .csv files containing normal and imbalanced data, \n",
    "#takes first 5000 points from each, assigns label in separate array\n",
    "if choice == 1:\n",
    "    files_no = glob.glob('normal/*.csv')\n",
    "    folders_im = glob.glob('imbalance/*')\n",
    "    train_data = np.empty((382,no_points,8), float)\n",
    "    test_data = np.empty((382,no_points,8), float)\n",
    "    i=0\n",
    "    for f_on in files_no:\n",
    "        source_data = np.loadtxt(f_on, delimiter=\",\")\n",
    "        train_data[i,:,:] = source_data[0:no_points,:]\n",
    "        test_data[i,:,:] = source_data[no_points:no_points*2,:]\n",
    "        i=i+1\n",
    "\n",
    "    for folder in folders_im:\n",
    "        files_im = glob.glob( folder +'/*.csv')\n",
    "        for f_im in files_im:\n",
    "            source_data = np.loadtxt(f_im, delimiter=\",\")\n",
    "            train_data[i,:,:] = source_data[0:no_points//6,:]\n",
    "            test_data[i,:,:] = source_data[no_points//6:no_points//6*2,:]\n",
    "            i=i+1\n",
    "    \n",
    "    train_reshaped = np.reshape(train_data,(train_data.shape[0],-1))\n",
    "    test_reshaped = np.reshape(test_data,(train_data.shape[0],-1))\n",
    "    \n",
    "    np.savetxt(\"train_data_equal.txt\", train_reshaped)\n",
    "    np.savetxt(\"test_data_equal.txt\", test_reshaped)\n",
    "    \n",
    "    train_label = np.zeros(49)\n",
    "    train_label = np.append(train_label, np.ones(333), axis=0)\n",
    "    test_label = np.zeros(49)\n",
    "    test_label = np.append(test_label, np.ones(333), axis=0)\n",
    "\n",
    "    np.savetxt(\"train_label.txt\", train_label)\n",
    "    np.savetxt(\"test_label.txt\", test_label)\n",
    "    \n",
    "    print(\"Finished parsing the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "828eed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382,)\n",
      "Loaded data files...\n"
     ]
    }
   ],
   "source": [
    "#Loads train/test data from .txt files to np arrays\n",
    "if choice == 0:    \n",
    "    train_reload = np.loadtxt(\"train_data.txt\")\n",
    "    test_reload = np.loadtxt(\"test_data.txt\")\n",
    "\n",
    "    train_data = np.reshape(train_reload,(train_reload.shape[0],train_reload.shape[1]//8, 8))\n",
    "    test_data = np.reshape(test_reload,(test_reload.shape[0],test_reload.shape[1]//8, 8))\n",
    "    \n",
    "\n",
    "    train_label = np.loadtxt('train_label.txt').astype(int)\n",
    "    test_label = np.loadtxt('test_label.txt').astype(int)\n",
    "    \n",
    "    print(train_label.shape)\n",
    "    print(\"Loaded data files...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f08e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the batch of 5000 into ten batches of 500\n",
    "train_data = train_data.reshape(train_data.shape[0]*10,int(no_points/10),8, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0]*10, int(no_points/10), 8, 1)\n",
    "train_label = np.zeros(490)\n",
    "train_label = np.append(train_label, np.ones(3330), axis=0)\n",
    "test_label = np.zeros(490)\n",
    "test_label = np.append(test_label, np.ones(3330), axis=0)\n",
    "\n",
    "# #Change label arrays to categorical\n",
    "# train_label = np_utils.to_categorical(train_label)\n",
    "# test_label = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d4261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load prediction data\n",
    "pred_load = np.loadtxt(\"pred_data.txt\")\n",
    "pred_data = np.reshape(pred_load,(pred_load.shape[0],pred_load.shape[1]//8, 8, 1))\n",
    "pred_label = np.loadtxt(\"pred_label.txt\")\n",
    "# pred_label = np_utils.to_categorical(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16153052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 498, 6, 30)        300       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 89640)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               11474048  \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 11,484,717\n",
      "Trainable params: 11,484,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(30, (3, 3), input_shape=(500, 8, 1), activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe16fe62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 [==============================] - 3s 13ms/step - loss: 0.2786 - accuracy: 0.8848 - val_loss: 0.0901 - val_accuracy: 0.9681\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0717 - accuracy: 0.9782 - val_loss: 0.0915 - val_accuracy: 0.9681\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0482 - accuracy: 0.9843 - val_loss: 0.0671 - val_accuracy: 0.9775\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.0645 - val_accuracy: 0.9775\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0268 - accuracy: 0.9878 - val_loss: 0.0488 - val_accuracy: 0.9832\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.0589 - val_accuracy: 0.9804\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0328 - accuracy: 0.9902 - val_loss: 0.0704 - val_accuracy: 0.9730\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0296 - accuracy: 0.9903 - val_loss: 0.0534 - val_accuracy: 0.9830\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0064 - accuracy: 0.9975 - val_loss: 0.0576 - val_accuracy: 0.9801\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0066 - accuracy: 0.9969 - val_loss: 0.1211 - val_accuracy: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15f1df30550>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_label, validation_data=(test_data, test_label), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab4fd957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9683\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7eae0b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.9694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15962617099285126, 0.9693717360496521]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(pred_data, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3f3c831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3820,)\n",
      "0.9730366492146597\n"
     ]
    }
   ],
   "source": [
    "# print(pred_data.shape)\n",
    "# print(pred_label.shape)\n",
    "prediction = model.predict_classes(pred_data)\n",
    "prediction = prediction.reshape(prediction.shape[0])\n",
    "print(prediction.shape)\n",
    "print(1-sum(prediction != pred_label)/3820)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdd8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu-new]",
   "language": "python",
   "name": "conda-env-tf-gpu-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
